{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a67e8b",
   "metadata": {},
   "source": [
    "Must measure the accuracy of model only on the validation set.\n",
    "\n",
    "Over-fitting: If you train for too long, with not enough data, you'll see the accuracy of your model start to get worse. Training model in such way that it remembers specific features of the input data, rather than generalising well to data not seen during training.\n",
    "\n",
    "epoch: looked at every image once - and to print out information on how the model is doing (i.e. erorr rate on the validation set). \n",
    "\n",
    "architecture: the template of te model that we're trying to fit, the actual mathematical funciton that we're passing the input data and parameters to.\n",
    "\n",
    "metric: function that measures the quality of the models predictions using the validation set, printed after each epoch.\n",
    "\n",
    "loss: function that the computer/algorithm is using to update the parameters. Cannot use metrics (i.e. erorr rate) as change is erorr rate will not change the parameters sometimes. \n",
    "\n",
    "\n",
    "split data into third set callde the training set - thus avoiding to optimise model for validation set. this set will be used after project is finished. \n",
    "\n",
    "\n",
    "Transfer learning: using a pretrained modl for a task different to what is it was originally trained for (i.e. learn.fine_tune(1)). Start with these weights from pre-trained model and train more epoch's on relevant data to come up with good model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02406f",
   "metadata": {},
   "source": [
    " p-value is the probability of the an observed (or more extereme) result assuming that the null hypothesis is true. \n",
    " \n",
    " Should not use p-values\n",
    " \n",
    " - p-values can indicate how incompatible that data are with a specified statistical model. \n",
    " \n",
    " - p-values do not meauser the probability that the studided hypothesis is ture, or the proabability that the data were produced by random chance alone. \n",
    " \n",
    " - conclusions and business or policy dectisions should not be based only on whether a p-value passes a specific threshold. \n",
    " \n",
    " - Does not measure the size of an effect or the importance of a result. \n",
    " \n",
    " - Does not provide a good measure of evidence regarding a model or hypothesis. \n",
    " \n",
    " \n",
    " relationship could be random - if we ran some simulation on random normal distribution of data points generated. \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54ef4b",
   "metadata": {},
   "source": [
    "#### Howard's drivetrain approach \n",
    "\n",
    "Going beyond predictions and turning predictions into decisions/actions. \n",
    "https://www.oreilly.com/radar/drivetrain-approach-data-products/\n",
    "http://radar.oreilly.com/2012/03/drivetrain-approach-data-products.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to data loaders \n",
    "\n",
    "bears = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock), # tuples of indendent(images)/dependednt (type of bear) variables\n",
    "    get_items=get_image_files, #path and returns a list of all the images in the path\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    get_y=parent_label, #function to call to create the labels in dataset\n",
    "    item_tfms=Resize(128)) #each image will be resized to 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We don't have a lot of data for our problem (150 pictures of each \n",
    "#sort of bear at most), so to train our model, we'll use \n",
    "#RandomResizedCrop with an image size of 224 px, which is fairly \n",
    "#standard for image classification, and default aug_transforms:\n",
    "\n",
    "bears = bears.new(\n",
    "    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n",
    "    batch_tfms=aug_transforms())\n",
    "dls = bears.dataloaders(path)\n",
    "\n",
    "# create the learner and fine-tune\n",
    "learn = vision_learner(dls, resnet18, metrics=error_rate)\n",
    "learn.fine_tune(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the confusion matrix to see where model is making mistakes\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13dda1c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-883e37ea7704>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# export model as pickle file to reuse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#export.pkl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# check if file exits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_exts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "# export model as pickle file to reuse\n",
    "learn.export() #export.pkl\n",
    "# check if file exits\n",
    "path = Path()\n",
    "path.ls(file_exts='.pkl')\n",
    "\n",
    "learn_inf = load_learner(path/'export.pkl') # load model in\n",
    "\n",
    "learn_inf.predict('images/grizzly.jpg') # predict image\n",
    "\n",
    "learn_inf.dls.vocab # understand the prediction categories and mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320263e",
   "metadata": {},
   "source": [
    "### How to avoid disaster \n",
    "\n",
    "**out-of-domain data** - That is to say, there may be data that our model sees in production which is very different to what it saw during training. There isn't really a complete technical solution to this problem; instead, we have to be careful about our approach to rolling out the technology.\n",
    "\n",
    "**domain shift** - where the type of data that our model sees changes over time. For instance, an insurance company may use a deep learning model as part of its pricing and risk algorithm, but over time the types of customers that the company attracts, and the types of risks they represent, may change so much that the original training data is no longer relevant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d1ad95",
   "metadata": {},
   "source": [
    " when releasing model into production - make sure you do it gradually\n",
    " \n",
    " - manual process: run model in parallel with domain expert to check alongside\n",
    " - limit scope deployment: only run on cohort of users, small amount with human supervision. \n",
    " - gradual expansion: expanding to wide customers with good reporting systems needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb15730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
